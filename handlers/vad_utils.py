import torch
import torchaudio
from silero_vad import get_speech_timestamps, read_audio
import os
from silero_vad import VoiceActivityDetector

model = VoiceActivityDetector("silero_vad.jit")

def extract_voice_segments(input_path, output_path, chat_id=None, send_message=None):
    try:
        if send_message:
            send_message(chat_id, "üß† –û—Ç–±–∏—Ä–∞—é —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ—Å —Å –ø–æ–º–æ—â—å—é VAD...")

        print(f"[VAD] –ó–∞–≥—Ä—É–∂–∞—é –∞—É–¥–∏–æ: {input_path}")
        model.reset_states()

        wav = read_audio(input_path, sampling_rate=16000)

        print("[VAD] –ò—â—É –≥–æ–ª–æ—Å–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")
        speech_timestamps = get_speech_timestamps(wav, model='silero_vad', sampling_rate=16000)

        if not speech_timestamps:
            err = "‚ùå –†–µ—á—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –ø–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–æ–µ –≤–∏–¥–µ–æ"
            if send_message:
                send_message(chat_id, err)
            raise ValueError(err)

        print(f"[VAD] –ù–∞–π–¥–µ–Ω–æ {len(speech_timestamps)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ä–µ—á–∏")

        chunks = []
        for t in speech_timestamps:
            start = int(t['start'] * 16)
            end = int(t['end'] * 16)
            chunks.append(wav[start:end])

        speech = torch.cat(chunks)

        print(f"[VAD] –°–æ—Ö—Ä–∞–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {output_path}")
        torchaudio.save(output_path, speech.unsqueeze(0), 16000)

        if send_message:
            send_message(chat_id, "‚úÖ –†–µ—á—å —É—Å–ø–µ—à–Ω–æ –≤—ã–¥–µ–ª–µ–Ω–∞")

        return output_path

    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏: {str(e)}"
        print(f"[VAD] {error_msg}")
        if send_message:
            send_message(chat_id, error_msg)
        raise
